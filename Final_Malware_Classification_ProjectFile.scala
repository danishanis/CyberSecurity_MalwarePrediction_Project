// Databricks notebook source
// SQL Library:

import sqlContext.implicits._
import org.apache.spark.sql._
import org.apache.spark.sql.functions.{col, lit, when}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.functions.spark_partition_id
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.Row


//Machine Learning Library
import org.apache.spark.ml.classification.{BinaryLogisticRegressionSummary, LogisticRegression}
import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator
import org.apache.spark.ml.evaluation.RegressionEvaluator
import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder, TrainValidationSplit}
import org.apache.spark.ml.classification.BinaryLogisticRegressionSummary
import org.apache.spark.ml.{Pipeline, PipelineModel}
import org.apache.spark.ml.feature.StringIndexer
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.ml.feature.VectorIndexer
import org.apache.spark.ml.linalg.{Vector, Vectors}
import org.apache.spark.ml.linalg.{Matrix, Vectors}
import org.apache.spark.ml.stat.Correlation

//ML Lib Library
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics



// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC # 1.0 Objective of this project:
// MAGIC
// MAGIC To identify malware in microsoft devices....

// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC # 2.0 Uploading the Data:

// COMMAND ----------

// Feature Dataframe:
val dfX = spark.read.format("csv")
  .option("header", "true")
  .option("inferSchema", "false")
  .option("sep", ",")
  .load("xMalware.csv")
  //.load("/FileStore/tables/xMalware.csv")
// COMMAND ----------

dfX.printSchema()

// COMMAND ----------

// Target Dataframe:

val dfY = spark.read.format("csv")
  .option("header", "true")
  .option("inferSchema", "false")
  .option("sep", ",")
  .load("yMalware.csv")
  //.load("/FileStore/tables/yMalware.csv")

display(dfY)

// COMMAND ----------



// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC # 3.0 Data Cleaning

// COMMAND ----------

// MAGIC %md
// MAGIC ## 3.1 Filling null values:

// COMMAND ----------

val dfXnn = dfX.na.fill("--")

// COMMAND ----------

print((dfXnn.count()))

// COMMAND ----------

// MAGIC %md
// MAGIC ## 3.2 Inspecting Columns:

// COMMAND ----------

// Assessing the percentage of nan in each column (part 1: examining every column)

val columns = dfXnn.columns

for(column <- columns){
  val dftest = dfXnn.groupBy(column).count()
  dftest.filter(column + "== '--'").show()
}

// COMMAND ----------

// The following columns are heavilly skewed and hence are dropped:
dfXnn.groupBy("PuaMode").count().show
dfXnn.groupBy("Census_ProcessorClass").count().show
dfXnn.groupBy("Census_IsFlightsDisabled").count().show
dfXnn.groupBy("Census_IsFlightingInternal").count().show
dfXnn.groupBy("Census_IsWIMBootEnabled").count().show
dfXnn.groupBy("IsBeta").count().show
dfXnn.groupBy("AutoSampleOptIn").count().show
dfXnn.groupBy("SMode").count().show
dfXnn.groupBy("IsBeta").count().show
dfXnn.groupBy("AutoSampleOptIn").count().show
dfXnn.groupBy("SMode").count().show
dfXnn.groupBy("Census_InternalBatteryType").count().show
dfXnn.groupBy("Census_IsPortableOperatingSystem").count().show
dfXnn.groupBy("Census_DeviceFamily").count().show
dfXnn.groupBy("UacLuaenable").count().show


// COMMAND ----------

// MAGIC %md
// MAGIC ## 3.3 Dropping unecessary columns

// COMMAND ----------

val initialDf= dfXnn

// val dfX2=initialDf.drop("PuaMode").drop("Census_ProcessorClass")
//   .drop("Census_IsWIMBootEnabled").drop("IsBeta")
//   .drop("Census_IsFlightsDisabled").drop("Census_IsFlightingInternal")
//   .drop("AutoSampleOptIn").drop("SMode")
//   .drop("Census_IsPortableOperatingSystem").drop("Census_DeviceFamily")
//   .drop("UacLuaenable").drop("Census_IsVirtualDevice")
//   .drop("Platform").drop("Census_OSSkuName")
//   .drop("Census_OSInstallLanguageIdentifier").drop("Processor")
//   .drop("PuaMode").drop("MachineIdentifier") // pua mode appears twice

// display(dfX2)

// COMMAND ----------

val cols = initialDf.columns
var x = cols(1)

// COMMAND ----------

// val xMalware_table = initialDf.createOrReplaceTempView("xMalware_table")

for (c <- cols){
  println(c)
spark.newSession().sql("SELECT 100.0 * count("+c+") / count(1) as PercentNotNull FROM xmalware ").show()
}

// COMMAND ----------

// Logic:
// The following code returns the normalized frequency of all the categorical entries in each of the columns. When any one particular entry occupies more than 98% of the entries they will be dropped. The assumption is that those variables do not have an effect on the outcome of the malware detection.

for (c <- cols){
  println(c)
spark.newSession().sql("select a."+c+", a.sub_count/sum(a.sub_count) over () as frequency from (select "+c+", count(*) as sub_count from xmalware group by "+c+")a order by frequency desc").show()
}

// COMMAND ----------

// Coloumns with less than 70% of data present and with more then 98% entries dominating the columns are dropped. This brings the total to 64 fields.

val dfX2=initialDf
  .drop("DefaultBrowsersIdentifier")
  .drop("OrganizationIdentifier")
  .drop("PuaMode")
  .drop("Census_ProcessorClass")
  .drop("Census_InternalBatteryType")
  .drop("Census_IsFlightingInternal")
  .drop("Census_ThresholdOptIn")
  .drop("Census_IsWIMBootEnabled")
  .drop("IsBeta")
  .drop("ProductName")
  .drop("IsSxsPassiveMode")
  .drop("HasTpm")
  .drop("AutoSampleOptIn")
  .drop("UacLuaenable")
  .drop("Census_IsFlightsDisabled")
  .drop("Census_IsVirtualDevice")


display(dfX2)

// COMMAND ----------

// Here we assess how many distinct values each column contains
// We do that to assess if adding dummy variables will trigger the curse of dimensionality (or a large number of columns)
// Also, we do that to assess if the column has only one distinct value, which means it adds no value to our analysis
dfX2.columns

val dfX3 = dfX2.select(dfX2.columns.map(c => countDistinct(col(c)).alias(c)): _*)
display(dfX3)

// The columns we might have to deal with (dimensionality curse):
// "AvSigVersion"  --> 3779
// "AVProductStatesIdentifier"  --> 2004
// "CountryIdentifier" --> 221 (number might be acceptable,very important metric to keep)
// "CityIdentifier" --> 14908 (very important metric to keep)
// "GeoNameIdentifier" --> 245
// "LocaleEnglishNameIdentifier" -->191
// "OsBuildLab" --> 345
// "IeVerIdentifier" -- 134
// "Census_OEMNameIdentifier" --> 833
// "Census_OEMModelIdentifier" --> 15172
// "Census_ProcessorModelIdentifier" --> 1672
// "Census_PrimaryDiskTotalCapacity" --> 447
// "Census_SystemVolumeTotalCapacity" --> 41790
// "Census_TotalPhysicalRAM" --> 172
// "Census_InternalPrimaryDiagonalDisplaySizeInInches" --> 356
// "Census_InternalPrimaryDisplayResolutionHorizontal" --> 146
// "Census_InternalPrimaryDisplayResolutionVertical" --> 167
// "Census_InternalBatteryNumberOfCharges" --> 1598
// "Census_OSVersion" --> 251
// "Census_OSBuildRevision" --> 216
// "Census_FirmwareManufacturerIdentifier" --> 171
// "Census_FirmwareVersionIdentifier" --> 11821

// COMMAND ----------

// MAGIC %md
// MAGIC ## 3.4 Improving column values

// COMMAND ----------

// DBTITLE 1,Column: SmartScreen
// As shwon below, SmartScreen contains overlaping classes:
dfX2.groupBy("SmartScreen").count().show(30)

// COMMAND ----------

// Hence we will combine some classess together:

var dfXS1 = dfX2
  .withColumn("SmartScreen", when(col("SmartScreen").equalTo("--"), "Does Not Exist")
                             .otherwise(col("SmartScreen")
                    ))

var dfXS2 = dfXS1
  .withColumn("SmartScreen", when(col("SmartScreen").equalTo("off"), "Off")
                             .otherwise(col("SmartScreen")
                    ))

var dfXS3 = dfXS2
  .withColumn("SmartScreen", when(col("SmartScreen").equalTo("&#x02"), "2")
                             .otherwise(col("SmartScreen")
                    ))

var dfXS4 = dfXS3
  .withColumn("SmartScreen", when(col("SmartScreen").equalTo("&#x01"), "1")
                             .otherwise(col("SmartScreen")
                    ))

var dfXS5 = dfXS4
  .withColumn("SmartScreen", when(col("SmartScreen").equalTo("on"), "On")
                             .otherwise(col("SmartScreen")
                    ))

// COMMAND ----------

dfXS5.groupBy("SmartScreen").count().show

// COMMAND ----------

// DBTITLE 1,Doing some more data exploration and  column dropping
//Previous results needed from amine!!*

// Dropping based on the previous results
val dfXS6 = (dfXS5.drop("Census_IsVirtualDevice")
                          .drop("MachineIdentifier")
                          .drop("Census_IsFlightsDisabled")
                          .drop("AutoSampleOptIn")
                          .drop("SMode")
                          .drop("Census_IsPortableOperatingSystem")
                          .drop("Census_DeviceFamily")
                          .drop("UacLuaenable"))

// COMMAND ----------



// COMMAND ----------



// COMMAND ----------

// DBTITLE 1,Final Feature Dataframe
val dfXfinal = dfXS6
  .filter($"EngineVersion" =!= "--").filter($"ProductName" =!= "--")
  .filter($"AppVersion" =!= "--").filter($"AvSigVersion" =!= "--")
  .filter($"RtpStateBitfield" =!= "--").filter($"IsSxsPassiveMode" =!= "--")
  .filter($"AVProductStatesIdentifier" =!= "--")
  .filter($"AVProductsInstalled" =!= "--").filter($"AVProductsEnabled" =!= "--")
  .filter($"HasTpm" =!= "--").filter($"CountryIdentifier" =!= "--")
  .filter($"CityIdentifier" =!= "--").filter($"OrganizationIdentifier" =!= "--")
  .filter($"GeoNameIdentifier" =!= "--").filter($"LocaleEnglishNameIdentifier" =!= "--")
  .filter($"OsVer" =!= "--").filter($"OsBuild" =!= "--")
  .filter($"OsSuite" =!= "--").filter($"OsPlatformSubRelease" =!= "--")
  .filter($"OsBuildLab" =!= "--").filter($"SkuEdition" =!= "--")
  .filter($"IsProtected" =!= "--").filter($"IeVerIdentifier" =!= "--")
  .filter($"Firewall" =!= "--").filter($"Census_MDC2FormFactor" =!= "--")

display(dfXfinal)

// COMMAND ----------



// COMMAND ----------

print((dfXfinal.count()))

// COMMAND ----------

// DBTITLE 1,Final Joined Dataframe
val dfJoined = dfXfinal.join(dfY,Seq("_c0")) // join based on unique _c0 values

// display(dfJoined)

// COMMAND ----------

print((dfJoined.count())) //this dataframe contains all features and corresponding target after dropping columns and rows above.//

// COMMAND ----------



// COMMAND ----------

// MAGIC %md
// MAGIC
// MAGIC # 4.0 EDA and Feature Analysis

// COMMAND ----------



// COMMAND ----------



// COMMAND ----------



// COMMAND ----------

// MAGIC %md
// MAGIC # 5.0 Feature Engineering

// COMMAND ----------

// casting Census_SystemVolumeTotalCapacity as double and creating 10 category splits based on the difference between the minimum and maximum

val dfJoined8 = dfJoined.withColumn("Census_SystemVolumeTotalCapacity",col("Census_SystemVolumeTotalCapacity").cast("Double"))

// COMMAND ----------

dfJoined8.select("Census_SystemVolumeTotalCapacity").show(5)


// COMMAND ----------

// Getting the min and max of Census_SystemVolumeTotalCapacity column
dfJoined8.describe("Census_SystemVolumeTotalCapacity").show()

// COMMAND ----------

// Converting into categories:
// estimates from results in table above, we get:
val min_vol_capacity = 11261.0
val max_vol_capacity = 17168788.0
val delta_ = (max_vol_capacity - min_vol_capacity)/10.0

// COMMAND ----------

// DBTITLE 1,More tweaking req.
val dfJoined9 = (dfJoined8.withColumn("Census_SystemVolumeTotalCapacity",
     when($"Census_SystemVolumeTotalCapacity" <= min_vol_capacity + delta_, "1").
     when($"Census_SystemVolumeTotalCapacity" >= min_vol_capacity + delta_ && $"Census_SystemVolumeTotalCapacity" <= min_vol_capacity + 2.0*delta_, "2").
     when($"Census_SystemVolumeTotalCapacity" >= min_vol_capacity + 2.0 * delta_ && $"Census_SystemVolumeTotalCapacity" <= min_vol_capacity + 3.0 * delta_, "3").
     when($"Census_SystemVolumeTotalCapacity" >= min_vol_capacity + 3.0 * delta_ && $"Census_SystemVolumeTotalCapacity" <= min_vol_capacity + 4.0 * delta_, "4").
     when($"Census_SystemVolumeTotalCapacity" >= min_vol_capacity + 4.0 * delta_ && $"Census_SystemVolumeTotalCapacity" <= min_vol_capacity + 5.0 * delta_, "5").
     when($"Census_SystemVolumeTotalCapacity" >= min_vol_capacity + 5.0 * delta_ && $"Census_SystemVolumeTotalCapacity" <= min_vol_capacity + 6.0 * delta_, "6").
     when($"Census_SystemVolumeTotalCapacity" >= min_vol_capacity + 6.0 * delta_ && $"Census_SystemVolumeTotalCapacity" <= min_vol_capacity + 7.0 * delta_, "7").
     when($"Census_SystemVolumeTotalCapacity" >= min_vol_capacity + 7.0 * delta_ && $"Census_SystemVolumeTotalCapacity" <= min_vol_capacity + 8.0 * delta_, "8").
     when($"Census_SystemVolumeTotalCapacity" >= min_vol_capacity + 8.0 * delta_ && $"Census_SystemVolumeTotalCapacity" <= min_vol_capacity + 9.0 * delta_, "9").
     when($"Census_SystemVolumeTotalCapacity" >= min_vol_capacity + 9.0 * delta_, "10")
                                         ))

// COMMAND ----------

//dfJoined5.select("Census_SystemVolumeTotalCapacity") (this result is much better than ~45000 distinct columns we had before)
dfJoined9.groupBy("Census_SystemVolumeTotalCapacity").count().count()

// COMMAND ----------

val dfJoined = dfJoined9
// Displaying final dataframe
display(dfJoined)

// COMMAND ----------

print((dfJoined.count()))

// COMMAND ----------

dfJoined.groupBy("Census_SystemVolumeTotalCapacity").count().show

// COMMAND ----------

// MAGIC %md
// MAGIC # 6.0 Baseline Model
// MAGIC
// MAGIC ### Problem formulation and model definition

// COMMAND ----------

// DBTITLE 1,Untitled
val df_features = dfJoined.columns
  .filterNot(_.contains("_c0"))
  .filterNot(_.contains("HasDetections"))

val encodedFeatures = df_features.flatMap{ name =>

  val stringIndexer = new StringIndexer ()
    .setInputCol(name)
    .setOutputCol(name + "_Index")
    .setHandleInvalid("skip")

  Array(stringIndexer)
}

val pipeline1 = new Pipeline().setStages(encodedFeatures)

val indexer_model = pipeline1.fit(dfXfinal)

val df_transformed = indexer_model.transform(dfXfinal)

display(df_transformed)

// COMMAND ----------

print(df_transformed.count())

// COMMAND ----------

// DBTITLE 1,Vector Assembler - Sparse feature matrix
val stringFeatures = df_transformed.columns.filter(_.contains("Index")).toArray

val vectorAssembler = new VectorAssembler()
  .setInputCols(stringFeatures)
  .setOutputCol("features")

val vectorIndexer = new VectorIndexer()
  .setInputCol("features")
  .setOutputCol("indexed_features")
  .setMaxCategories(20)

val pipeline2 = new Pipeline().setStages(Array(vectorAssembler,vectorIndexer))


val indexer_model2 = pipeline2.fit(df_transformed)

val df_sparsefeatures = indexer_model2.transform(df_transformed)

// display(df_sparsefeatures)

// COMMAND ----------

print(df_sparsefeatures.count())

// COMMAND ----------

// DBTITLE 1,Converting sparse feature column to dense feature column
val sparseToDense = udf((v: Vector) => v.toDense)

val df_densefeatures = df_sparsefeatures.withColumn("dense_indexed_features", sparseToDense($"indexed_features"))

// COMMAND ----------

val df_temp= df_densefeatures.join(dfY,Seq("_c0")).select("HasDetections", "dense_indexed_features")

// Renaming columns for lr & changing label to integer type
val df_final = df_temp.withColumn("HasDetectionsTmp", df_temp("HasDetections").cast(IntegerType))
  .drop("HasDetections")
  .withColumnRenamed("HasDetectionsTmp","label")
  .withColumnRenamed("dense_indexed_features","features")
  .select("label", "features")


display(df_final)

// COMMAND ----------

val splits = df_final.randomSplit(Array(0.7, 0.3))

val training = splits(0).cache()

val test = splits(1)

// COMMAND ----------

// Create a LogisticRegression instance. This instance is an Estimator.
val lr = new LogisticRegression()

// Learn a LogisticRegression model. This uses the parameters stored in lr.
val model1 = lr.fit(training)

// Make predictions on test data using the Transformer.transform() method.
// LogisticRegression.transform will only use the 'features' column.
val model1_predictions = model1.transform(test)
model1_predictions.show()

// COMMAND ----------

// Since model1 is a Model (i.e., a Transformer produced by an Estimator),
// we can view the parameters it used during fit().
// This prints the parameter (name: value) pairs, where names are unique IDs for this

// LogisticRegression instance.
println(s"Model 1 was fit using parameters: ${model1.parent.extractParamMap}")

// Print out the parameters, documentation, and any default values.
println(s"LogisticRegression parameters:\n ${lr.explainParams()}\n")

// COMMAND ----------



// COMMAND ----------

// MAGIC %md
// MAGIC # 7.0 Model Evaluation & Improvement
// MAGIC
// MAGIC ### Iterative model improvement & model evaluation and metrics

// COMMAND ----------

// MAGIC %md
// MAGIC ## Logistic Regression

// COMMAND ----------

val evaluator = new BinaryClassificationEvaluator()
      .setLabelCol("label")
      .setRawPredictionCol("rawPrediction")

evaluator.evaluate(model1_predictions)

// //Binary Classification Evaluator uses Area under the ROC as the default performance metric.
// print("the area under ROC for train set is{}".format(evaluator.evaluate(training)))
// print("the area under ROC for test set is{}".format(evaluator.evaluate(test)))

// COMMAND ----------

//Define Parameter Grid for hyperparameter tuning
val paramGrid = new ParamGridBuilder()
  .addGrid(lr.regParam, Array(0.01, 0.5, 2.0))
  .addGrid(lr.maxIter, Array(10, 100, 1000))
  .addGrid(lr.threshold, Array(0.1,0.2,0.4))
  .addGrid(lr.fitIntercept)
  .addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0))
  .build()


// COMMAND ----------

//Set up TrainValidation Split

val trainValidationSplit = new TrainValidationSplit()
  .setEstimator(lr)
  .setEvaluator(new BinaryClassificationEvaluator)
  .setEstimatorParamMaps(paramGrid)
  .setTrainRatio(0.8)

//run trainvalidation split
val modeltv = trainValidationSplit.fit(training)

// COMMAND ----------

//make predictions on test data with best parameters, as per Train Validation Split algorithm
val modeltv_predictions = modeltv.transform(test)

//apply best model from Train Validation Split to training data to get an idea of the training error
val modeltv_training = modeltv.transform(training)

// COMMAND ----------

val lp1 = modeltv_predictions.select( "label", "prediction")
val counttotal1 = modeltv_predictions.count()
val correct1 = lp.filter($"label" === $"prediction").count()
val wrong1 = lp.filter(not($"label" === $"prediction")).count()
val truep1 = lp.filter($"prediction" === 0.0).filter($"label" === $"prediction").count()
val falseN1 = lp.filter($"prediction" === 0.0).filter(not($"label" === $"prediction")).count()
val falseP1 = lp.filter($"prediction" === 1.0).filter(not($"label" === $"prediction")).count()
val ratioWrong1=wrong1.toDouble/counttotal1.toDouble
val ratioCorrect1=correct1.toDouble/counttotal1.toDouble

// COMMAND ----------

//show results of Training Validation Split
//println(s"Model 1 was fit using parameters: ${modeltv_predictions.parent.extractParamMap}")

modeltv_predictions.show()

// COMMAND ----------

//get test error
println(s"Testing Error: ${evaluator.evaluate(modeltv_predictions)}")

//get train error
println(s"Training Error: ${evaluator.evaluate(modeltv_training)}")

// COMMAND ----------

val areaunderROC = evaluator.setMetricName("areaUnderROC").evaluate(modeltv_predictions)

// COMMAND ----------

val areaunderPR = evaluator.setMetricName("areaUnderPR").evaluate(modeltv_predictions)

// COMMAND ----------

val tvmodel_summary = modeltv_predictions.summary()
tvmodel_summary.show()

// COMMAND ----------

//set up the cross validation
val cv = new CrossValidator()
  .setEstimator(lr)
  .setEvaluator(new BinaryClassificationEvaluator)
  .setEstimatorParamMaps(paramGrid)
  .setNumFolds(3)

// COMMAND ----------

//run the cross validation by fitting the baseline LR Model to the Training data with Cross Validation
val cvModel = cv.fit(training)

// COMMAND ----------

import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel}

val lrModel = cvModel.bestModel.asInstanceOf[LogisticRegressionModel]

println(s"LR Model coefficients:\n${lrModel.coefficients.toArray.mkString("\n")}")

// COMMAND ----------

val paramscv = cvModel.bestModel.asInstanceOf[LogisticRegressionModel].extractParamMap()


// COMMAND ----------

//Now apply the best model from the Cross Validation to the test data. Store results in a new value called cvModel_Predictions
// n/a ? val LRModel= cvModel.transform(training)
//val cvModel_predictions = cvModel.transform(test)

//also apply best model from CV to training data to get training error.
val cvModel_training = cvModel.transform(training)
val cvModel_test = cvModel.transform(test)

// COMMAND ----------

val lrModeltv = modeltv.bestModel.asInstanceOf[LogisticRegressionModel]

println(s"LR Model with TrainValidation Split coefficients:\n${lrModeltv.coefficients.toArray.mkString("\n")}")


// COMMAND ----------

val lrModel = modeltv.bestModel.asInstanceOf[LogisticRegressionModel]

println(s"LR Model coefficients:\n${lrModel.coefficients.toArray.mkString("\n")}")

// COMMAND ----------

training.show()

// COMMAND ----------

val lp = cvModel_test.select( "label", "prediction")
val counttotal = cvModel_test.count()
val correct = lp.filter($"label" === $"prediction").count()
val wrong = lp.filter(not($"label" === $"prediction")).count()
val truep = lp.filter($"prediction" === 0.0).filter($"label" === $"prediction").count()
val falseN = lp.filter($"prediction" === 0.0).filter(not($"label" === $"prediction")).count()
val falseP = lp.filter($"prediction" === 1.0).filter(not($"label" === $"prediction")).count()
val ratioWrong=wrong.toDouble/counttotal.toDouble
val ratioCorrect=correct.toDouble/counttotal.toDouble

// COMMAND ----------

val features1  = cvModel.bestModel.asInstanceOf[LogisticRegressionModel].getFeaturesCol

val params1  = cvModel.bestModel.asInstanceOf[LogisticRegressionModel].extractParamMap()

// COMMAND ----------

//get train error
println(s"Training Error: ${evaluator.evaluate(cvModel_training)}")

println(s"Test Error: ${evaluator.evaluate(cvModel_test)}")

// COMMAND ----------

val areaunderROC_cv = evaluator.setMetricName("areaUnderROC").evaluate(cvModel_training)
val areaunderPR_cv = evaluator.setMetricName("areaUnderPR").evaluate(cvModel_training)

val areaunderROC_cv_test = evaluator.setMetricName("areaUnderROC").evaluate(cvModel_test)
val areaunderPR_cv_test = evaluator.setMetricName("areaUnderPR").evaluate(cvModel_test)

// COMMAND ----------

//show results of Cross Validation
//println(s"Model 1 was fit using parameters: ${cvModel_predictions.parent.extractParamMap}")

cvModel_training.show()

// COMMAND ----------

// LogisticRegression instance.
println(s"Cross Validated Model was fit using parameters: ${cvModel.parent.extractParamMap}")

// Print out the parameters, documentation, and any default values.
println(s"LogisticRegression parameters:\n ${cvModel.explainParams()}\n")

// COMMAND ----------

//create RDD to use MLLIB library
val PR = model1_predictions.select("label", "probability").map{ case Row(l:Double, p:Vector) => (p(1),l)}
val PR2 = PR.rdd


// COMMAND ----------

val metrics_rdd = new BinaryClassificationMetrics(PR2)

// Precision by threshold
//val precision = metrics_rdd.precisionByThreshold
//precision.foreach { case (t, p) =>
//  println(s"Threshold: $t, Precision: $p")
//}

// F-measure
val f1Score = metrics_rdd.fMeasureByThreshold
f1Score.foreach { case (t, f) =>
  println(s"Threshold: $t, F-score: $f, Beta = 1")
}

//precision
//val precision = metrics_rdd.precisionByThreshold

//precision.collect.foreach { case (t,p) =>
//  println (s"Threshold: $t, Precision: $p")
//}

//val PRC = metrics_rdd.pr
//PRC.collect().foreach(println)

// COMMAND ----------

// MAGIC %md
// MAGIC ## Gradient-boosted Tree Classifier

// COMMAND ----------

import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}

val gbt = new GBTClassifier().setFeatureSubsetStrategy("auto")

val model2 = gbt.fit(training)

val model2_predictions = model2.transform(test)
model2_predictions.show()

// COMMAND ----------

//Evaluating the gradient-boosted Tree Classifier on the test set
val evaluator_tree = new MulticlassClassificationEvaluator()
  .setLabelCol("label")
  .setPredictionCol("prediction")
  .setMetricName("accuracy")
val accuracy_tree = evaluator_tree.evaluate(model2_predictions)
println(s"Test Error = ${1.0 - accuracy_tree}")
println("Test set accuracy = " + accuracy_tree)

// COMMAND ----------

// MAGIC %md
// MAGIC ## Random Forest Classifier

// COMMAND ----------

import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}

// Create a RandomForest instance.
val rf = new RandomForestClassifier().setNumTrees(2000)


val model_rf = rf.fit(training)


// COMMAND ----------

//Evaluating the Random Forest Tree Classifier on the test set
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
val model_rf_predictions = model_rf.transform(test)
val evaluator_rf_test = new MulticlassClassificationEvaluator()
  .setLabelCol("label")
  .setPredictionCol("prediction")
  .setMetricName("accuracy")
val accuracy_rf_tree = evaluator_rf_test.evaluate(model_rf_predictions)
println(s"Test Error = ${1.0 - accuracy_rf_tree}")
println("Test set accuracy = " + accuracy_rf_tree)

// COMMAND ----------

//Evaluating the Random Forest Tree Classifier on the training set
val model_rf_training = model_rf.transform(training)
val evaluator_rf_training = new MulticlassClassificationEvaluator()
  .setLabelCol("label")
  .setPredictionCol("prediction")
  .setMetricName("accuracy")
val accuracy_rf_training = evaluator_rf_training.evaluate(model_rf_training)
println(s"Test Error = ${1.0 - accuracy_rf_tree}")
println("Training set accuracy = " + accuracy_rf_training)

// COMMAND ----------



// COMMAND ----------

// MAGIC %md
// MAGIC # 8.0 Model Explanation and Feature Contribution Analysis

// COMMAND ----------




// COMMAND ----------



// COMMAND ----------



// COMMAND ----------
